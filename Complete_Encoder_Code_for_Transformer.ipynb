{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOxaRJQ9rdCxvSu5CYI8mI1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2003Yash/complete-encoder/blob/main/Complete_Encoder_Code_for_Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " import tensorflow as tf\n",
        "from tensorflow.keras.layers import Layer, Dense, Dropout, LayerNormalization\n",
        "import math\n",
        "\n",
        "def scaled_dot_product(q, k, v, mask=None):\n",
        "    d_k = tf.shape(q)[-1]\n",
        "    scaled = tf.matmul(q, k, transpose_b=True) / tf.sqrt(tf.cast(d_k, tf.float32))\n",
        "    print(f\"scaled.shape : {scaled.shape}\")\n",
        "    if mask is not None:\n",
        "        print(f\"-- ADDING MASK of shape {mask.shape} --\")\n",
        "        scaled += mask\n",
        "    attention = tf.nn.softmax(scaled, axis=-1)\n",
        "    values = tf.matmul(attention, v)\n",
        "    return values, attention\n",
        "\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.d_model = d_model #512\n",
        "        self.num_heads = num_heads #8\n",
        "        self.head_dim = d_model // num_heads #512/8 = 64\n",
        "        self.qkv_layer = tf.keras.layers.Dense(3 * d_model) #1536\n",
        "        self.linear_layer = tf.keras.layers.Dense(d_model) #512\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        batch_size, max_sequence_length, d_model = tf.shape(x) # 30 x 200 x 512\n",
        "        print(f\"x.shape: {x.shape}\")\n",
        "        qkv = self.qkv_layer(x) # 30 x 200 x 1536\n",
        "        print(f\"qkv.shape: {qkv.shape}\")\n",
        "        qkv = tf.reshape(qkv, (batch_size, max_sequence_length, self.num_heads, 3 * self.head_dim)) # 30 x 200 x 8 x 192\n",
        "        print(f\"qkv.shape after reshape: {qkv.shape}\")\n",
        "        qkv = tf.transpose(qkv, perm=[0, 2, 1, 3]) # transpose -> 30 x 8 x 200 x 192\n",
        "        print(f\"qkv.shape after transpose: {qkv.shape}\")\n",
        "        q, k, v = tf.split(qkv, 3, axis=-1) # splitiing for q,k,v i.e.., 192 = 64x3 => 30 x 8 x 200 x 64 each\n",
        "        print(f\"q shape: {q.shape}, k shape: {k.shape}, v shape: {v.shape}\")\n",
        "        values, attention = scaled_dot_product(q, k, v, mask) # find the attention vectors from q,k,v and mask it if needed\n",
        "        print(f\"values.shape: {values.shape}, attention.shape: {attention.shape}\")\n",
        "        values = tf.reshape(values, (batch_size, max_sequence_length, self.num_heads * self.head_dim)) # 3o x 200 x 500\n",
        "        print(f\"values.shape after reshape: {values.shape}\")\n",
        "        out = self.linear_layer(values)\n",
        "        print(f\"out.shape: {out.shape}\")\n",
        "        return out\n",
        "\n",
        "class LayerNormalization(tf.keras.layers.Layer):\n",
        "    def __init__(self, parameters_shape, eps=1e-5):\n",
        "        super(LayerNormalization, self).__init__()\n",
        "        self.parameters_shape = parameters_shape\n",
        "        self.eps = eps\n",
        "        self.gamma = self.add_weight(shape=parameters_shape, initializer='ones', trainable=True)\n",
        "        self.beta = self.add_weight(shape=parameters_shape, initializer='zeros', trainable=True)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        dims = list(range(len(self.parameters_shape)))\n",
        "        mean = tf.reduce_mean(inputs, axis=dims, keepdims=True)\n",
        "        print(f\"Mean ({mean.shape})\")\n",
        "        var = tf.reduce_mean(tf.square(inputs - mean), axis=dims, keepdims=True)\n",
        "        std = tf.sqrt(var + self.eps)\n",
        "        print(f\"Standard Deviation  ({std.shape})\")\n",
        "        y = (inputs - mean) / std\n",
        "        print(f\"y: {y.shape}\")\n",
        "        out = self.gamma * y + self.beta\n",
        "        print(f\"self.gamma: {self.gamma.shape}, self.beta: {self.beta.shape}\")\n",
        "        print(f\"out: {out.shape}\")\n",
        "        return out\n",
        "\n",
        "class PositionwiseFeedForward(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, hidden, drop_prob=0.1):\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        self.linear1 = tf.keras.layers.Dense(hidden) # 2048 neurons\n",
        "        self.linear2 = tf.keras.layers.Dense(d_model) # 512 neurons these values are again stored in matirx i.e.., 30x200x_512_ these neurons just porcess vlaues with lernable params no classification here\n",
        "        self.relu = tf.keras.layers.ReLU()\n",
        "        self.dropout = tf.keras.layers.Dropout(rate=drop_prob)\n",
        "\n",
        "    def call(self, x, training=False):\n",
        "        x = self.linear1(x) # 30 x 200 x 2048\n",
        "        print(f\"x after first linear layer: {x.shape}\")\n",
        "        x = self.relu(x)\n",
        "        print(f\"x after activation: {x.shape}\")\n",
        "        x = self.dropout(x, training=training)\n",
        "        print(f\"x after dropout: {x.shape}\")\n",
        "        x = self.linear2(x)  # 30 x x200 x 512\n",
        "        print(f\"x after 2nd linear layer: {x.shape}\")\n",
        "        return x\n",
        "\n",
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob):\n",
        "      #using values of architecture data we will create the transformer architecture in this constructor\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.attention = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
        "        self.norm1 = LayerNormalization(parameters_shape=[d_model])\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate=drop_prob)\n",
        "        self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob) # after getting attention matix then we put them in ffn  #check that postionwise class in above\n",
        "        self.norm2 = LayerNormalization(parameters_shape=[d_model])\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate=drop_prob)\n",
        "\n",
        "    def call(self, x, training=False):\n",
        "        residual_x = x\n",
        "        print(\"------- ATTENTION 1 ------\")\n",
        "        x = self.attention(x, mask=None) #30 x 200 x 512\n",
        "        print(\"------- DROPOUT 1 ------\")\n",
        "        x = self.dropout1(x, training=training) #30 x 200 x 512\n",
        "        print(\"------- ADD AND LAYER NORMALIZATION 1 ------\")\n",
        "        x = self.norm1(x + residual_x)\n",
        "        residual_x = x #30 x 200 x 512\n",
        "        print(\"------- ATTENTION 2 ------\")\n",
        "        x = self.ffn(x) #30 x 200 x 512\n",
        "        print(\"------- DROPOUT 2 ------\")\n",
        "        x = self.dropout2(x, training=training) #30 x 200 x 512\n",
        "        print(\"------- ADD AND LAYER NORMALIZATION 2 ------\")\n",
        "        x = self.norm2(x + residual_x) #30 x 200 x 512\n",
        "        return x # this x is just trained word embedding with encoder and ffn to capture the best context\n",
        "\n",
        "class Encoder(Layer):\n",
        "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob, num_layers):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.layers = [EncoderLayer(d_model, ffn_hidden, num_heads, drop_prob)\n",
        "                       for _ in range(num_layers)] #this will create encoder 5 times as specified\n",
        "\n",
        "    def call(self, x, training=False):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, training=training)\n",
        "        return x\n",
        "\n",
        "d_model = 512 #length of word embedding\n",
        "num_heads = 8 #number of attention heads\n",
        "drop_prob = 0.1 #dropout rate\n",
        "batch_size = 30 #batch size\n",
        "max_sequence_length = 200 #max sequence length\n",
        "ffn_hidden = 2048 #hidden layer size (feed forward network)\n",
        "num_layers = 5 #number of encoder layers -> we pass thorugh the input through many copies of encoders to get a desired output -> if data is too complex increase the number\n",
        "\n",
        "\n",
        "encoder = Encoder(d_model, ffn_hidden, num_heads, drop_prob, num_layers)\n",
        "\n",
        "x = tf.random.normal((batch_size, max_sequence_length, d_model))  # includes positional encoding\n",
        "out = encoder(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3HhRUOhAEVUQ",
        "outputId": "f27b4c99-a4e5-40db-f9bb-fb10dfa20d43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------- ATTENTION 1 ------\n",
            "------- ATTENTION 1 ------\n",
            "------- ATTENTION 1 ------\n",
            "------- ATTENTION 1 ------\n",
            "x.shape: (30, 200, 512)\n",
            "qkv.shape: (30, 200, 1536)\n",
            "qkv.shape after reshape: (30, 200, 8, 192)\n",
            "qkv.shape after transpose: (30, 8, 200, 192)\n",
            "q shape: (30, 8, 200, 64), k shape: (30, 8, 200, 64), v shape: (30, 8, 200, 64)\n",
            "scaled.shape : (30, 8, 200, 200)\n",
            "values.shape: (30, 8, 200, 64), attention.shape: (30, 8, 200, 200)\n",
            "values.shape after reshape: (30, 200, 512)\n",
            "out.shape: (30, 200, 512)\n",
            "------- DROPOUT 1 ------\n",
            "------- ADD AND LAYER NORMALIZATION 1 ------\n",
            "Mean ((1, 200, 512))\n",
            "Standard Deviation  ((1, 200, 512))\n",
            "y: (30, 200, 512)\n",
            "self.gamma: (512,), self.beta: (512,)\n",
            "out: (30, 200, 512)\n",
            "------- ATTENTION 2 ------\n",
            "x after first linear layer: (30, 200, 2048)\n",
            "x after activation: (30, 200, 2048)\n",
            "x after dropout: (30, 200, 2048)\n",
            "x after 2nd linear layer: (30, 200, 512)\n",
            "x after first linear layer: (30, 200, 2048)\n",
            "x after activation: (30, 200, 2048)\n",
            "x after dropout: (30, 200, 2048)\n",
            "x after 2nd linear layer: (30, 200, 512)\n",
            "------- DROPOUT 2 ------\n",
            "------- ADD AND LAYER NORMALIZATION 2 ------\n",
            "Mean ((1, 200, 512))\n",
            "Standard Deviation  ((1, 200, 512))\n",
            "y: (30, 200, 512)\n",
            "self.gamma: (512,), self.beta: (512,)\n",
            "out: (30, 200, 512)\n",
            "------- ATTENTION 1 ------\n",
            "------- ATTENTION 1 ------\n",
            "x.shape: (30, 200, 512)\n",
            "qkv.shape: (30, 200, 1536)\n",
            "qkv.shape after reshape: (30, 200, 8, 192)\n",
            "qkv.shape after transpose: (30, 8, 200, 192)\n",
            "q shape: (30, 8, 200, 64), k shape: (30, 8, 200, 64), v shape: (30, 8, 200, 64)\n",
            "scaled.shape : (30, 8, 200, 200)\n",
            "values.shape: (30, 8, 200, 64), attention.shape: (30, 8, 200, 200)\n",
            "values.shape after reshape: (30, 200, 512)\n",
            "out.shape: (30, 200, 512)\n",
            "------- DROPOUT 1 ------\n",
            "------- ADD AND LAYER NORMALIZATION 1 ------\n",
            "Mean ((1, 200, 512))\n",
            "Standard Deviation  ((1, 200, 512))\n",
            "y: (30, 200, 512)\n",
            "self.gamma: (512,), self.beta: (512,)\n",
            "out: (30, 200, 512)\n",
            "------- ATTENTION 2 ------\n",
            "x after first linear layer: (30, 200, 2048)\n",
            "x after activation: (30, 200, 2048)\n",
            "x after dropout: (30, 200, 2048)\n",
            "x after 2nd linear layer: (30, 200, 512)\n",
            "x after first linear layer: (30, 200, 2048)\n",
            "x after activation: (30, 200, 2048)\n",
            "x after dropout: (30, 200, 2048)\n",
            "x after 2nd linear layer: (30, 200, 512)\n",
            "------- DROPOUT 2 ------\n",
            "------- ADD AND LAYER NORMALIZATION 2 ------\n",
            "Mean ((1, 200, 512))\n",
            "Standard Deviation  ((1, 200, 512))\n",
            "y: (30, 200, 512)\n",
            "self.gamma: (512,), self.beta: (512,)\n",
            "out: (30, 200, 512)\n",
            "------- ATTENTION 1 ------\n",
            "------- ATTENTION 1 ------\n",
            "x.shape: (30, 200, 512)\n",
            "qkv.shape: (30, 200, 1536)\n",
            "qkv.shape after reshape: (30, 200, 8, 192)\n",
            "qkv.shape after transpose: (30, 8, 200, 192)\n",
            "q shape: (30, 8, 200, 64), k shape: (30, 8, 200, 64), v shape: (30, 8, 200, 64)\n",
            "scaled.shape : (30, 8, 200, 200)\n",
            "values.shape: (30, 8, 200, 64), attention.shape: (30, 8, 200, 200)\n",
            "values.shape after reshape: (30, 200, 512)\n",
            "out.shape: (30, 200, 512)\n",
            "------- DROPOUT 1 ------\n",
            "------- ADD AND LAYER NORMALIZATION 1 ------\n",
            "Mean ((1, 200, 512))\n",
            "Standard Deviation  ((1, 200, 512))\n",
            "y: (30, 200, 512)\n",
            "self.gamma: (512,), self.beta: (512,)\n",
            "out: (30, 200, 512)\n",
            "------- ATTENTION 2 ------\n",
            "x after first linear layer: (30, 200, 2048)\n",
            "x after activation: (30, 200, 2048)\n",
            "x after dropout: (30, 200, 2048)\n",
            "x after 2nd linear layer: (30, 200, 512)\n",
            "x after first linear layer: (30, 200, 2048)\n",
            "x after activation: (30, 200, 2048)\n",
            "x after dropout: (30, 200, 2048)\n",
            "x after 2nd linear layer: (30, 200, 512)\n",
            "------- DROPOUT 2 ------\n",
            "------- ADD AND LAYER NORMALIZATION 2 ------\n",
            "Mean ((1, 200, 512))\n",
            "Standard Deviation  ((1, 200, 512))\n",
            "y: (30, 200, 512)\n",
            "self.gamma: (512,), self.beta: (512,)\n",
            "out: (30, 200, 512)\n",
            "------- ATTENTION 1 ------\n",
            "------- ATTENTION 1 ------\n",
            "x.shape: (30, 200, 512)\n",
            "qkv.shape: (30, 200, 1536)\n",
            "qkv.shape after reshape: (30, 200, 8, 192)\n",
            "qkv.shape after transpose: (30, 8, 200, 192)\n",
            "q shape: (30, 8, 200, 64), k shape: (30, 8, 200, 64), v shape: (30, 8, 200, 64)\n",
            "scaled.shape : (30, 8, 200, 200)\n",
            "values.shape: (30, 8, 200, 64), attention.shape: (30, 8, 200, 200)\n",
            "values.shape after reshape: (30, 200, 512)\n",
            "out.shape: (30, 200, 512)\n",
            "------- DROPOUT 1 ------\n",
            "------- ADD AND LAYER NORMALIZATION 1 ------\n",
            "Mean ((1, 200, 512))\n",
            "Standard Deviation  ((1, 200, 512))\n",
            "y: (30, 200, 512)\n",
            "self.gamma: (512,), self.beta: (512,)\n",
            "out: (30, 200, 512)\n",
            "------- ATTENTION 2 ------\n",
            "x after first linear layer: (30, 200, 2048)\n",
            "x after activation: (30, 200, 2048)\n",
            "x after dropout: (30, 200, 2048)\n",
            "x after 2nd linear layer: (30, 200, 512)\n",
            "x after first linear layer: (30, 200, 2048)\n",
            "x after activation: (30, 200, 2048)\n",
            "x after dropout: (30, 200, 2048)\n",
            "x after 2nd linear layer: (30, 200, 512)\n",
            "------- DROPOUT 2 ------\n",
            "------- ADD AND LAYER NORMALIZATION 2 ------\n",
            "Mean ((1, 200, 512))\n",
            "Standard Deviation  ((1, 200, 512))\n",
            "y: (30, 200, 512)\n",
            "self.gamma: (512,), self.beta: (512,)\n",
            "out: (30, 200, 512)\n",
            "------- ATTENTION 1 ------\n",
            "------- ATTENTION 1 ------\n",
            "x.shape: (30, 200, 512)\n",
            "qkv.shape: (30, 200, 1536)\n",
            "qkv.shape after reshape: (30, 200, 8, 192)\n",
            "qkv.shape after transpose: (30, 8, 200, 192)\n",
            "q shape: (30, 8, 200, 64), k shape: (30, 8, 200, 64), v shape: (30, 8, 200, 64)\n",
            "scaled.shape : (30, 8, 200, 200)\n",
            "values.shape: (30, 8, 200, 64), attention.shape: (30, 8, 200, 200)\n",
            "values.shape after reshape: (30, 200, 512)\n",
            "out.shape: (30, 200, 512)\n",
            "------- DROPOUT 1 ------\n",
            "------- ADD AND LAYER NORMALIZATION 1 ------\n",
            "Mean ((1, 200, 512))\n",
            "Standard Deviation  ((1, 200, 512))\n",
            "y: (30, 200, 512)\n",
            "self.gamma: (512,), self.beta: (512,)\n",
            "out: (30, 200, 512)\n",
            "------- ATTENTION 2 ------\n",
            "x after first linear layer: (30, 200, 2048)\n",
            "x after activation: (30, 200, 2048)\n",
            "x after dropout: (30, 200, 2048)\n",
            "x after 2nd linear layer: (30, 200, 512)\n",
            "x after first linear layer: (30, 200, 2048)\n",
            "x after activation: (30, 200, 2048)\n",
            "x after dropout: (30, 200, 2048)\n",
            "x after 2nd linear layer: (30, 200, 512)\n",
            "------- DROPOUT 2 ------\n",
            "------- ADD AND LAYER NORMALIZATION 2 ------\n",
            "Mean ((1, 200, 512))\n",
            "Standard Deviation  ((1, 200, 512))\n",
            "y: (30, 200, 512)\n",
            "self.gamma: (512,), self.beta: (512,)\n",
            "out: (30, 200, 512)\n"
          ]
        }
      ]
    }
  ]
}